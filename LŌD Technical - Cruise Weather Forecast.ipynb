{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c0e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ipyleaflet requests transformers torch\n",
    "#%pip install 'accelerate>=0.26.0'\n",
    "#%pip install -U bitsandbytes\n",
    "# get access token from huggingface??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acbdb5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophiakim/miniconda3/envs/virtual_env/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/sophiakim/miniconda3/envs/virtual_env/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from ipyleaflet import Map as LeafletMap, DrawControl\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer, GPTJForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfbe558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once data has been processed, we check if there are any invalid data (i.e. marker is on land)\n",
    "def is_valid_data(data):\n",
    "    hourly_data = data['hourly']\n",
    "\n",
    "    for hourly_entry in hourly_data.items():\n",
    "        values = hourly_entry[1]\n",
    "        if all(value is None for value in values):\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13491885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophiakim/miniconda3/envs/virtual_env/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Initialize gpu for model to run on\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Initialize llm\n",
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "# pipe = pipeline(model=model_name, torch_dtype=torch.float16, tokenizer=model_name, batch_size=1)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "model = GPTJForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/gpt-j-6B\",\n",
    "    revision=\"float16\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# pipe = pipeline(task=\"text-generation\", model=model_name, tokenizer=tokenizer)\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2b5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data before passing to llm\n",
    "def create_prompt(data):\n",
    "    prompt = \"Give me a time when it is safe to dock my ship based on the following data: \\n\"\n",
    "    prompt += str(data)\n",
    "    # clean_data += \"Hours: \" + hourly_data['time'][0] + \" to \" + hourly_data['time'][23] + \"\\n\"\n",
    "    # clean_data += \"Latitude: \" + str(data['latitude']) + \", Longitude \" + str(data['longitude']) + \"\\n\"\n",
    "    # clean_data += \"Hourly Wind Wave Heights: \" + \", \".join([str(height) for height in data['hourly']['wind_wave_height']]) + \" meters.\\n\"\n",
    "    # clean_data += \"Hourly Wind Wave Directions: \" + \", \".join([str(direction) for direction in data['hourly']['wind_wave_direction']]) + \" kts.\\n\"\n",
    "    # clean_data += \"Hourly Wind Wave Periods: \" + \", \".join([str(period) for period in data['hourly']['wind_wave_period']]) + \" s.\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab96ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_summary(prompt):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\", padding='longest')\n",
    "    # adding pad token ID\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    # move input to gpu\n",
    "    input = {key: value.to(device) for key, value in input.items()}\n",
    "    # print(\"Inputs Shape: \" + str(input['input_ids'].shape) + \", Device: \" + str(input['input_ids'].device))\n",
    "    # print(input)\n",
    "    output = model.generate(\n",
    "        input['input_ids'],\n",
    "        attention_mask=input['attention_mask'],\n",
    "        max_length=1000,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    print(output)\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # result = pipe(prompt, max_new_tokens=200)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06f0d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event handler when user pins marker on map\n",
    "def handle_marker(event, action, geo_json):\n",
    "    coordinates = geo_json['geometry']['coordinates']\n",
    "    longitude, latitude = str(coordinates[0]), str(coordinates[1])\n",
    "\n",
    "    # Fetch sea conditions from Open-Meteo API\n",
    "    url = \"https://marine-api.open-meteo.com/v1/marine?latitude=\" + latitude + \"&longitude=\" + longitude + \"&hourly=wind_wave_height,wind_wave_direction,wind_wave_period&timezone=auto&forecast_days=1\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if is_valid_data(data):\n",
    "            prompt = create_prompt(data)\n",
    "            summary = retrieve_summary(prompt)\n",
    "            print(summary[0]['generated_text'])\n",
    "        else:\n",
    "            print(\"Please try again\")\n",
    "    else:\n",
    "        print(\"Error fetching data from Open-Meteo API: \" + str(response.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88158c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a150ea606cfb43429bee845388f90c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[20.0, 0.0], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoom_out_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m, in \u001b[0;36mhandle_marker\u001b[0;34m(event, action, geo_json)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid_data(data):\n\u001b[1;32m     12\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m create_prompt(data)\n\u001b[0;32m---> 13\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(summary[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mretrieve_summary\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(\"Inputs Shape: \" + str(input['input_ids'].shape) + \", Device: \" + str(input['input_ids'].device))\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# print(input)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[1;32m     17\u001b[0m result \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.10/site-packages/transformers/generation/utils.py:1993\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1990\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1992\u001b[0m device \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m-> 1993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_has_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001b[39;00m\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m   1997\u001b[0m     \u001b[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001b[39;00m\n\u001b[1;32m   1998\u001b[0m     \u001b[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.10/site-packages/transformers/generation/utils.py:1842\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_special_tokens\u001b[0;34m(self, generation_config, kwargs_has_attention_mask, device)\u001b[0m\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1837\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1838\u001b[0m     )\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():  \u001b[38;5;66;03m# Checks that depend on tensor-dependent control flow\u001b[39;00m\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1841\u001b[0m         eos_token_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1842\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[43misin_mps_friendly\u001b[49m\u001b[43m(\u001b[49m\u001b[43melements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39many()\n\u001b[1;32m   1843\u001b[0m     ):\n\u001b[1;32m   1844\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m kwargs_has_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs_has_attention_mask:\n\u001b[1;32m   1845\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1846\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe attention mask is not set and cannot be inferred from input because pad token is same as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1847\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meos token. As a consequence, you may observe unexpected behavior. Please pass your input\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1848\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attention_mask` to obtain reliable results.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1849\u001b[0m             )\n",
      "File \u001b[0;32m~/miniconda3/envs/virtual_env/lib/python3.10/site-packages/transformers/pytorch_utils.py:325\u001b[0m, in \u001b[0;36misin_mps_friendly\u001b[0;34m(elements, test_elements)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03mSame as `torch.isin` without flags, but MPS-friendly. We can remove this function when we stop supporting\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03mtorch <= 2.3. See https://github.com/pytorch/pytorch/issues/77764#issuecomment-2067838075\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    and False otherwise\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m elements\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal_than_2_4:\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elements\u001b[38;5;241m.\u001b[39mtile(\u001b[43mtest_elements\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39meq(test_elements\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misin(elements, test_elements)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "map_center = [20.0, 0.0]\n",
    "m = LeafletMap(center=map_center, zoom=2, scroll_wheel_zoom=True)\n",
    "\n",
    "draw_control = DrawControl(\n",
    "    marker={'shapeOptions': {}},\n",
    "    polyline={},\n",
    "    polygon={},\n",
    "    circle={},\n",
    "    circlemarker={},\n",
    ")\n",
    "m.add_control(draw_control)\n",
    "draw_control.on_draw(handle_marker)\n",
    "\n",
    "# Display map\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13103ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a7c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
